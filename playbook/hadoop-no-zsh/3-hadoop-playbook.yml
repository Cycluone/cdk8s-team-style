- hosts: hadoops
  remote_user: root
  vars:
    java_install_folder: "/usr/local"
    hadoop_file_name: "hadoop-3.1.3.tar.gz"
    lzo_file_name: "hadoop-lzo-0.4.20.jar"
    java_home_path: "/usr/local/jdk1.8.0_261"
    hadoop_home_path: "/usr/local/hadoop-3.1.3"
    hadoop_etc_path: "{{ hadoop_home_path }}/etc/hadoop"
  tasks:
    - name: update hosts
      blockinfile:
        path: /etc/hosts
        marker: ""
        block: |
          192.168.31.137 header1
          192.168.31.88 worker1
          192.168.31.237 worker2

    - name: copy tar.gz
      copy:
        src="/opt/software/{{ hadoop_file_name }}"
        dest="{{ java_install_folder }}"

    - name: create directory
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - "{{ hadoop_home_path }}/tmp"
        - /home/data/hadoop/hdfs/tmpdir

    - name: tar gz
      shell: "{{ item }}"
      args:
        chdir: "{{ java_install_folder }}"
      with_items:
        - tar zxvf {{ hadoop_file_name }}

    - name: set HADOOP_HOME
      blockinfile:
        path: /root/.bashrc
        marker: "#{mark} HADOOP HOME"
        block: |
          export HADOOP_HOME={{ hadoop_home_path }}
          export PATH=$PATH:$HADOOP_HOME/bin
          export PATH=$PATH:$HADOOP_HOME/sbin
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native"

    - name: source bashrc
      shell: source /root/.bashrc

    - name: remove gz
      file:
        path: "{ java_install_folder }}/{{ hadoop_file_name }}"
        state: absent


    - name: copy lzo
      copy:
        src="/opt/software/{{ lzo_file_name }}"
        dest="{{ hadoop_home_path }}/share/hadoop/common"



    - name: copy hadoop-env.sh
      copy:
        src="{{ hadoop_etc_path }}/hadoop-env.sh"
        dest="{{ hadoop_etc_path }}/hadoop-env.sh.back"
    - name: remove hadoop-env.sh
      file:
        path: "{{ hadoop_etc_path }}/hadoop-env.sh"
        state: absent
    - name: create hadoop-env.sh
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - hadoop-env.sh
    - name: set hadoop-env.sh
      blockinfile:
        path: "{{ hadoop_etc_path }}/hadoop-env.sh"
        marker: ""
        block: |
          export JAVA_HOME={{ java_home_path }}
          export HADOOP_PID_DIR={{ hadoop_home_path }}/tmp
          export HDFS_NAMENODE_USER=root
          export HDFS_DATANODE_USER=root
          export HDFS_SECONDARYNAMENODE_USER=root
          export YARN_RESOURCEMANAGER_USER=root
          export YARN_NODEMANAGER_USER=root
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/hadoop-env.sh"
        regexp: "^$"
        state: absent

    - name: copy yarn-env.sh
      copy:
        src="{{ hadoop_etc_path }}/yarn-env.sh"
        dest="{{ hadoop_etc_path }}/yarn-env.sh.back"
    - name: remove yarn-env.sh
      file:
        path: "{{ hadoop_etc_path }}/yarn-env.sh"
        state: absent
    - name: create yarn-env.sh
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - yarn-env.sh
    - name: set yarn-env.sh
      blockinfile:
        path: "{{ hadoop_etc_path }}/yarn-env.sh"
        marker: ""
        block: |
          export JAVA_HOME={{ java_home_path }}
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/yarn-env.sh"
        regexp: "^$"
        state: absent


    - name: copy core-site.xml
      copy:
        src="{{ hadoop_etc_path }}/core-site.xml"
        dest="{{ hadoop_etc_path }}/core-site.xml.back"
    - name: remove core-site.xml
      file:
        path: "{{ hadoop_etc_path }}/core-site.xml"
        state: absent
    - name: create core-site.xml
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - core-site.xml
    - name: set core-site.xml
      blockinfile:
        path: "{{ hadoop_etc_path }}/core-site.xml"
        marker: ""
        block: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <!-- 指定NameNode的地址 -->
              <property>
                  <name>fs.defaultFS</name>
                  <value>hdfs://header1:9820</value>
              </property>
              <!-- 指定hadoop数据的存储目录 -->
              <property>
                  <name>hadoop.tmp.dir</name>
                  <value>/home/data/hadoop/hdfs/tmpdir</value>
              </property>

              <!-- 在网页界面访问数据使用的用户名。默认值是一个不真实存在的用户：dr.who，此用户权限很小，不能访问不同用户的数据。这保证了数据安全。 -->
              <!-- 想要获得更高的权限可以用你的 centos 当前用户名。参考：https://www.jianshu.com/p/fdc7d658ab38 -->
              <property>
                  <name>hadoop.http.staticuser.user</name>
                  <value>root</value>
              </property>
              <!-- 是否在HDFS中开启权限检查,默认为true -->
              <property>
                  <name>dfs.permissions.enabled</name>
                  <value>false</value>
              </property>
              <!-- 配置该root(superUser)允许通过代理访问的主机节点 -->
              <property>
                  <name>hadoop.proxyuser.root.hosts</name>
                  <value>*</value>
              </property>
              <!-- 配置该root(superUser)允许通过代理用户所属组 -->
              <property>
                  <name>hadoop.proxyuser.root.groups</name>
                  <value>*</value>
              </property>
              <!-- 配置该root(superUser)允许通过代理的用户-->
              <property>
                  <name>hadoop.proxyuser.root.groups</name>
                  <value>*</value>
              </property>
              <!--lzo 压缩配置 start-->
              <property>
                  <name>io.compression.codecs</name>
                  <value>
                      org.apache.hadoop.io.compress.GzipCodec,
                      org.apache.hadoop.io.compress.DefaultCodec,
                      org.apache.hadoop.io.compress.BZip2Codec,
                      org.apache.hadoop.io.compress.SnappyCodec,
                      com.hadoop.compression.lzo.LzoCodec,
                      com.hadoop.compression.lzo.LzopCodec
                  </value>
              </property>
              <property>
                  <name>io.compression.codec.lzo.class</name>
                  <value>com.hadoop.compression.lzo.LzoCodec</value>
              </property>
              <!--lzo 压缩配置 end-->
          </configuration>
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/core-site.xml"
        regexp: "^$"
        state: absent



    - name: copy hdfs-site.xml
      copy:
        src="{{ hadoop_etc_path }}/hdfs-site.xml"
        dest="{{ hadoop_etc_path }}/hdfs-site.xml.back"
    - name: remove hdfs-site.xml
      file:
        path: "{{ hadoop_etc_path }}/hdfs-site.xml"
        state: absent
    - name: create hdfs-site.xml
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - hdfs-site.xml
    - name: set hdfs-site.xml
      blockinfile:
        path: "{{ hadoop_etc_path }}/hdfs-site.xml"
        marker: ""
        block: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

          <configuration>
              <!-- namenode web端访问地址-->
              <property>
                  <name>dfs.namenode.http-address</name>
                  <value>header1:9870</value>
              </property>

              <!-- second namenode web端访问地址-->
              <property>
                  <name>dfs.namenode.secondary.http-address</name>
                  <value>worker2:9868</value>
              </property>

              <!-- HDFS副本的数量，默认值是 3 -->
              <!-- 这个参数跟下面的 $HADOOP_HOME/etc/hadoop/workers 中配置的节点数量要对应起来 -->
              <property>
                  <name>dfs.replication</name>
                  <value>3</value>
              </property>

              <!-- zchtodo 配置线程池数，默认是 10，根据集群的规模不同，这个值也要跟着优化 -->
              <property>
                  <name>dfs.namenode.handler.count</name>
                  <value>10</value>
              </property>
          </configuration>
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/hdfs-site.xml"
        regexp: "^$"
        state: absent


    - name: copy yarn-site.xml
      copy:
        src="{{ hadoop_etc_path }}/yarn-site.xml"
        dest="{{ hadoop_etc_path }}/yarn-site.xml.back"
    - name: remove yarn-site.xml
      file:
        path: "{{ hadoop_etc_path }}/yarn-site.xml"
        state: absent
    - name: create yarn-site.xml
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - yarn-site.xml
    - name: set yarn-site.xml
      blockinfile:
        path: "{{ hadoop_etc_path }}/yarn-site.xml"
        marker: ""
        block: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

          <configuration>
              <!-- 指定MR走shuffle -->
              <property>
                  <name>yarn.nodemanager.aux-services</name>
                  <value>mapreduce_shuffle</value>
              </property>

              <!-- 指定Yarn集群的管理者（ResourceManager）的地址-->
              <!-- yarn 启动就在这台机子上 -->
              <property>
                  <name>yarn.resourcemanager.hostname</name>
                  <value>worker1</value>
              </property>

              <!-- 环境变量的继承 -->
              <property>
                  <name>yarn.nodemanager.env-whitelist</name>
                  <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
              </property>

              <!-- zchtodo yarn容器允许分配的最大最小内存，根据集群硬件配置进行优化 -->
              <property>
                  <name>yarn.scheduler.minimum-allocation-mb</name>
                  <value>512</value>
              </property>

              <!-- zchtodo 单个任务可申请的最多物理内存量，默认是8192（MB） -->
              <property>
                  <name>yarn.scheduler.maximum-allocation-mb</name>
                  <value>4096</value>
              </property>

              <!-- zchtodo yarn容器允许管理的物理内存大小，根据集群硬件配置进行优化，默认是8192（MB） -->
              <property>
                  <name>yarn.nodemanager.resource.memory-mb</name>
                  <value>4096</value>
              </property>

              <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
              <property>
                  <name>yarn.nodemanager.pmem-check-enabled</name>
                  <value>false</value>
              </property>
              <property>
                  <name>yarn.nodemanager.vmem-check-enabled</name>
                  <value>false</value>
              </property>

              <!-- 开启日志聚集功能 -->
              <property>
                  <name>yarn.log-aggregation-enable</name>
                  <value>true</value>
              </property>

              <!-- 设置日志聚集服务器地址 -->
              <property>
                  <name>yarn.log.server.url</name>
                  <value>http://header1:19888/jobhistory/logs</value>
              </property>

              <!-- 设置日志保留时间为7天 -->
              <property>
                  <name>yarn.log-aggregation.retain-seconds</name>
                  <value>604800</value>
              </property>

              <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
              <property>
                  <name>yarn.nodemanager.pmem-check-enabled</name>
                  <value>false</value>
              </property>

              <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
              <property>
                  <name>yarn.nodemanager.vmem-check-enabled</name>
                  <value>false</value>
              </property>
          </configuration>
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/yarn-site.xml"
        regexp: "^$"
        state: absent



    - name: copy mapred-site.xml
      copy:
        src="{{ hadoop_etc_path }}/mapred-site.xml"
        dest="{{ hadoop_etc_path }}/mapred-site.xml.back"
    - name: remove mapred-site.xml
      file:
        path: "{{ hadoop_etc_path }}/mapred-site.xml"
        state: absent
    - name: create mapred-site.xml
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - mapred-site.xml
    - name: set mapred-site.xml
      blockinfile:
        path: "{{ hadoop_etc_path }}/mapred-site.xml"
        marker: ""
        block: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

          <configuration>
              <!-- 指定MapReduce程序运行在Yarn上 -->
              <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
              </property>

              <!-- 历史服务器端地址 -->
              <property>
                  <name>mapreduce.jobhistory.address</name>
                  <value>header1:10020</value>
              </property>

              <!-- 历史服务器web端地址 -->
              <property>
                  <name>mapreduce.jobhistory.webapp.address</name>
                  <value>header1:19888</value>
              </property>
          </configuration>
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/mapred-site.xml"
        regexp: "^$"
        state: absent




    - name: copy workers
      copy:
        src="{{ hadoop_etc_path }}/workers"
        dest="{{ hadoop_etc_path }}/workers.back"
    - name: remove workers
      file:
        path: "{{ hadoop_etc_path }}/workers"
        state: absent
    - name: create workers
      file:
        path="{{ hadoop_etc_path }}/{{ item }}"
        state=touch
        mode=777
      with_items:
        - workers
    - name: set workers
      blockinfile:
        path: "{{ hadoop_etc_path }}/workers"
        marker: ""
        block: |
          header1
          worker1
          worker2
    - name: remove blank lines blockinfile
      lineinfile :
        path: "{{ hadoop_etc_path }}/workers"
        regexp: "^$"
        state: absent
    - debug:
        msg: "workers 配置的是 datanode 工作的机器，datanode 主要是用来存放数据文件的"
    - debug:
        msg: "datanode 可以和 namenode 在同一台机器，也可以 namenode 单独一台机器"





















